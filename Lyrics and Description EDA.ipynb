{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "#!pip install emoji\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fatemehkhosravi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Add any additional import statements you need here\n",
    "\n",
    "#!pip install lexical-diversity\n",
    "from lexical_diversity import lex_div as ld\n",
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "#!pip install spacy\n",
    "#!pip install advertools\n",
    "import advertools as adv\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/fatemehkhosravi/Desktop/Assignment2/\"\n",
    "\n",
    "\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    num_unique_tokens = len(Counter(tokens).keys())\n",
    "    \n",
    "    lexical_diversity = ld.ttr(tokens)\n",
    "    \n",
    "    num_characters = 0\n",
    "    for word in tokens:\n",
    "        num_characters+= len(word)\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: Assertion allows us to test our codes when a condition returns true or debug our codes if AssertionError raise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "\n",
    "#I had to execute this line in my workspace terminal to remove error. \n",
    "#find . -name \"*.DS_Store\" -type f -delete\n",
    "\n",
    "lyrics_data= {\"artist\":[], \"song\":[],\"lyrics\": []}\n",
    "path1=data_location+lyrics_folder\n",
    "\n",
    "for artist in os.listdir(path1):\n",
    "    for song in os.listdir(os.path.join (path1+artist)):\n",
    "        song_name= song.split(\"_\")[1].split(\".\")[0]\n",
    "        with open (os.path.join(path1+artist, song),'r') as f:\n",
    "            lyric=f.read()\n",
    "        \n",
    "        lyrics_data['artist'].append(artist)\n",
    "        lyrics_data['song'].append(song_name)\n",
    "        lyrics_data['lyrics'].append(lyric)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the twitter data\n",
    "\n",
    "twitter_files = os.listdir(data_location + twitter_folder)\n",
    "desc_files = [f for f in twitter_files if \"followers_data\" in f]\n",
    "tweets_data= defaultdict(list)\n",
    "for f in desc_files :\n",
    "    artist = f.split(\"_\")[0]\n",
    "        \n",
    "    with open(data_location + twitter_folder + f,'r', encoding='utf8') as infile :\n",
    "        next(infile)\n",
    "        for idx, line in enumerate(infile.readlines()) :\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if len(line) == 7 :\n",
    "                tweets_data[artist].append(line[6])\n",
    "tweets_data= dict(tweets_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>csu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>I’m unemployed and live with my parents. MOOPS!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191118</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>singer of songs, type 1 diabetic, tired $jakel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191119</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>Dadx2/ Con-Arch/ Photographer/ DK #stemgrønnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191120</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>A year to change a life is still a year ✨😌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191121</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>Head of Consumer - Mango. Made in Melbourne. R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191122</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>Stand for what is right, even if you stand alone.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2191123 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 artist                                        description\n",
       "0                  cher           𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜\n",
       "1                  cher          163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡\n",
       "2                  cher                                                csu\n",
       "3                  cher  Writer @Washinformer @SpelmanCollege alumna #D...\n",
       "4                  cher    I’m unemployed and live with my parents. MOOPS!\n",
       "...                 ...                                                ...\n",
       "2191118  robynkonichiwa  singer of songs, type 1 diabetic, tired $jakel...\n",
       "2191119  robynkonichiwa  Dadx2/ Con-Arch/ Photographer/ DK #stemgrønnes...\n",
       "2191120  robynkonichiwa         A year to change a life is still a year ✨😌\n",
       "2191121  robynkonichiwa  Head of Consumer - Mango. Made in Melbourne. R...\n",
       "2191122  robynkonichiwa  Stand for what is right, even if you stand alone.\n",
       "\n",
       "[2191123 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison\n",
    "\n",
    "# convert dictionary file to dataframe \n",
    "tweets_df= pd.DataFrame(\n",
    "    [(k, val) for k, vals in tweets_data.items() for val in vals], \n",
    "    columns=['artist', 'description']\n",
    ")\n",
    "\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold to lowercase \n",
    "tweets_df['lower_description']= tweets_df['description'].apply(lambda x : x.strip().lower())\n",
    "\n",
    "\n",
    "# Remove punctuation characters\n",
    "def re_punctuations(text):\n",
    "       return''.join(ch for ch in text if ch not in punctuation)\n",
    "    \n",
    "tweets_df['no_punctuation'] = tweets_df['lower_description']. apply (re_punctuations)\n",
    "\n",
    "\n",
    "#Split on whitespace\n",
    "tweets_df['no_whitespace']=tweets_df['no_punctuation'].apply(lambda x : x.split())\n",
    "\n",
    "# Remove stopwords\n",
    "tweets_df['clean_text']=tweets_df['no_whitespace'].apply(lambda x: [item for item in x if item not in sw])\n",
    "\n",
    "\n",
    "# drop the useless columns\n",
    "tweets_df.drop(columns=['lower_description','no_punctuation' ,'no_whitespace'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>includemeout</td>\n",
       "      <td>\"Include Me Out\"\\n\\n\\n\\nIt is really very simp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>electric</td>\n",
       "      <td>\"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robyn</td>\n",
       "      <td>beach2k20</td>\n",
       "      <td>\"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn</td>\n",
       "      <td>lovekills</td>\n",
       "      <td>\"Love Kills\"\\n\\n\\n\\nIf you're looking for love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robyn</td>\n",
       "      <td>timemachine</td>\n",
       "      <td>\"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>cher</td>\n",
       "      <td>takeitfromtheboys</td>\n",
       "      <td>\"Take It From The Boys\"\\n\\n\\n\\nSo scared I nev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>cher</td>\n",
       "      <td>dreambaby</td>\n",
       "      <td>\"Dream Baby\"\\n\\n\\n\\nI found the boy for me\\nHe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>cher</td>\n",
       "      <td>pleasedonttellme</td>\n",
       "      <td>\"Please Don't Tell Me\"\\n\\n\\n\\nYa shook the ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>cher</td>\n",
       "      <td>ihopeyoufindit</td>\n",
       "      <td>\"I Hope You Find It\"\\n\\n\\n\\nThese clouds aren'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>cher</td>\n",
       "      <td>classified1a</td>\n",
       "      <td>\"Classified 1A\"\\n\\n\\n\\nI know now how much I l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist               song  \\\n",
       "0    robyn       includemeout   \n",
       "1    robyn           electric   \n",
       "2    robyn          beach2k20   \n",
       "3    robyn          lovekills   \n",
       "4    robyn        timemachine   \n",
       "..     ...                ...   \n",
       "415   cher  takeitfromtheboys   \n",
       "416   cher          dreambaby   \n",
       "417   cher   pleasedonttellme   \n",
       "418   cher     ihopeyoufindit   \n",
       "419   cher       classified1a   \n",
       "\n",
       "                                                lyrics  \n",
       "0    \"Include Me Out\"\\n\\n\\n\\nIt is really very simp...  \n",
       "1    \"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...  \n",
       "2    \"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...  \n",
       "3    \"Love Kills\"\\n\\n\\n\\nIf you're looking for love...  \n",
       "4    \"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan...  \n",
       "..                                                 ...  \n",
       "415  \"Take It From The Boys\"\\n\\n\\n\\nSo scared I nev...  \n",
       "416  \"Dream Baby\"\\n\\n\\n\\nI found the boy for me\\nHe...  \n",
       "417  \"Please Don't Tell Me\"\\n\\n\\n\\nYa shook the ove...  \n",
       "418  \"I Hope You Find It\"\\n\\n\\n\\nThese clouds aren'...  \n",
       "419  \"Classified 1A\"\\n\\n\\n\\nI know now how much I l...  \n",
       "\n",
       "[420 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert dictionary file to dataframe \n",
    "lyrics_df=pd.DataFrame.from_dict(lyrics_data)\n",
    "lyrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>includemeout</td>\n",
       "      <td>\"Include Me Out\"\\n\\n\\n\\nIt is really very simp...</td>\n",
       "      <td>[include, really, simple, single, pulse, repea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>electric</td>\n",
       "      <td>\"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...</td>\n",
       "      <td>[electric, electric, electric, natural, high, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robyn</td>\n",
       "      <td>beach2k20</td>\n",
       "      <td>\"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...</td>\n",
       "      <td>[beach, 2k20, wanna, go, gonna, get, ok, call,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn</td>\n",
       "      <td>lovekills</td>\n",
       "      <td>\"Love Kills\"\\n\\n\\n\\nIf you're looking for love...</td>\n",
       "      <td>[love, kills, youre, looking, love, get, heart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robyn</td>\n",
       "      <td>timemachine</td>\n",
       "      <td>\"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan...</td>\n",
       "      <td>[time, machine, hey, cant, believe, fit, threw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist          song                                             lyrics  \\\n",
       "0  robyn  includemeout  \"Include Me Out\"\\n\\n\\n\\nIt is really very simp...   \n",
       "1  robyn      electric  \"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...   \n",
       "2  robyn     beach2k20  \"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...   \n",
       "3  robyn     lovekills  \"Love Kills\"\\n\\n\\n\\nIf you're looking for love...   \n",
       "4  robyn   timemachine  \"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  [include, really, simple, single, pulse, repea...  \n",
       "1  [electric, electric, electric, natural, high, ...  \n",
       "2  [beach, 2k20, wanna, go, gonna, get, ok, call,...  \n",
       "3  [love, kills, youre, looking, love, get, heart...  \n",
       "4  [time, machine, hey, cant, believe, fit, threw...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fold to lowercase \n",
    "lyrics_df['lower_lyrics']= lyrics_df['lyrics'].apply(lambda x : x.strip().lower())\n",
    "\n",
    "\n",
    "# Remove punctuation characters\n",
    "def re_punctuations(text):\n",
    "       return''.join(ch for ch in text if ch not in punctuation)\n",
    "    \n",
    "lyrics_df['no_punctuation'] = lyrics_df['lower_lyrics']. apply (re_punctuations)\n",
    "\n",
    "\n",
    "#Split on whitespace\n",
    "lyrics_df['no_whitespace']=lyrics_df['no_punctuation'].apply(lambda x : x.split())\n",
    "\n",
    "# Remove stopwords\n",
    "lyrics_df['clean_text']=lyrics_df['no_whitespace'].apply(lambda x: [item for item in x if item not in sw])\n",
    "\n",
    "\n",
    "# drop the useless columns\n",
    "lyrics_df.drop(columns=['lower_lyrics','no_punctuation' ,'no_whitespace'], inplace=True)\n",
    "\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cher tweet:\n",
      "There are 2001090 tokens in the data.\n",
      "There are 1797531 unique tokens in the data.\n",
      "There are 106635955 characters in the data.\n",
      "The lexical diversity is 0.898 in the data.\n"
     ]
    }
   ],
   "source": [
    "#cher tweet\n",
    "\n",
    "tweets_cher=tweets_df.loc[tweets_df['artist']=='cher']\n",
    "\n",
    "# convert list of elements to string object\n",
    "description_cher=list(tweets_cher['clean_text'])\n",
    "\n",
    "description_cher_str=[]\n",
    "counter=0\n",
    "for text in description_cher:\n",
    "    myString = \" \".join(description_cher[counter])\n",
    "    description_cher_str.append(myString)\n",
    "    counter+=1\n",
    "\n",
    "print('cher tweet:')\n",
    "\n",
    "assert(descriptive_stats(description_cher_str, verbose=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robyn tweet:\n",
      "There are 190033 tokens in the data.\n",
      "There are 180322 unique tokens in the data.\n",
      "There are 10434591 characters in the data.\n",
      "The lexical diversity is 0.949 in the data.\n"
     ]
    }
   ],
   "source": [
    "#robyn tweet\n",
    "\n",
    "tweets_robyn=tweets_df.loc[tweets_df['artist']=='robynkonichiwa']\n",
    "\n",
    "# convert list of elements to string object\n",
    "description_robyn=list(tweets_robyn['clean_text'])\n",
    "\n",
    "description_robyn_str=[]\n",
    "counter=0\n",
    "for text in description_robyn:\n",
    "    myString = \" \".join(description_robyn[counter])\n",
    "    description_robyn_str.append(myString)\n",
    "    counter+=1\n",
    "\n",
    "print('robyn tweet:')\n",
    "\n",
    "assert(descriptive_stats(description_robyn_str, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35916 tokens in the data.\n",
      "There are 3703 unique tokens in the data.\n",
      "There are 172634 characters in the data.\n",
      "The lexical diversity is 0.103 in the data.\n"
     ]
    }
   ],
   "source": [
    "#cher lyrics\n",
    "\n",
    "song_cher=lyrics_df.loc[lyrics_df['artist']=='cher']\n",
    "song_cher=list(song_cher['clean_text'])\n",
    "\n",
    "# convert nested list to a flat list\n",
    "flat_list_cher = [element for innerList in song_cher for element in innerList]\n",
    "\n",
    "assert(descriptive_stats(flat_list_cher, verbose=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15227 tokens in the data.\n",
      "There are 2156 unique tokens in the data.\n",
      "There are 73787 characters in the data.\n",
      "The lexical diversity is 0.142 in the data.\n"
     ]
    }
   ],
   "source": [
    "# robyn lyrics\n",
    "\n",
    "song_robyn=lyrics_df.loc[lyrics_df['artist']=='robyn']\n",
    "song_robyn=list(song_robyn['clean_text'])\n",
    "\n",
    "# convert nested list to a flat list\n",
    "flat_list_robyn = [element for innerList in song_robyn for element in innerList]\n",
    "\n",
    "assert(descriptive_stats(flat_list_robyn, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: Since the frequency of stopwords in sentences is usually high, obtaining important information from these words does not give us much information. I think that when we left stopwords in the data, it is more likely that the top 5 words would be from stopwords. \n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: Lexical diversity for Cher’s lyrics is about 0.103 and for Robyn is about 0.142. Higher value means that the lyrics contain many different word types while lower value of lexical diversity refers to the repeated words or phrases. I am not familiar with Robyn’s songs, but I know Cher’s songs are usually similar to each other, simple and easy to memorize. 0.103 also proves my assumption about Cher's songs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"❤️\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🏳️\\u200d🌈', 3249),\n",
       " ('❤️', 2887),\n",
       " ('♥', 2886),\n",
       " ('✨', 2223),\n",
       " ('❤', 1771),\n",
       " ('🌈', 1417),\n",
       " ('💙', 809),\n",
       " ('💜', 739),\n",
       " ('🎶', 710),\n",
       " ('🖤', 615)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#robyn emogies\n",
    "\n",
    "emoji_robyn=tweets_df.loc[tweets_df['artist']=='robynkonichiwa','description']\n",
    "emoji_robyn=list(emoji_robyn)\n",
    "emoji_summary = adv.extract_emoji(emoji_robyn)\n",
    "\n",
    "# top 10 emojies \n",
    "emoji_summary['top_emoji'][0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('❤️', 45799),\n",
       " ('❤', 31788),\n",
       " ('🏳️\\u200d🌈', 30454),\n",
       " ('✨', 29468),\n",
       " ('♥', 28401),\n",
       " ('💙', 21379),\n",
       " ('🌊', 20223),\n",
       " ('🌈', 16898),\n",
       " ('💜', 16550),\n",
       " ('🇺🇸', 14686)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cher emogies\n",
    "\n",
    "emoji_cher=tweets_df.loc[tweets_df['artist']=='cher','description']\n",
    "emoji_cher=list(emoji_cher)\n",
    "emoji_summary = adv.extract_emoji(emoji_cher)\n",
    "\n",
    "# top 10 emojies \n",
    "emoji_summary['top_emoji'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BlackLivesMatter': 337,\n",
       " 'BLM': 306,\n",
       " 'blacklivesmatter': 208,\n",
       " '1': 199,\n",
       " 'music': 174,\n",
       " 'Music': 113,\n",
       " 'EDM': 86,\n",
       " 'LGBTQ': 75,\n",
       " 'TeamFollowBack': 59,\n",
       " 'blm': 56}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# robyn hashtags\n",
    "\n",
    "count_hashtag = Counter()\n",
    "for element in emoji_robyn:\n",
    "    for hashtags in re.findall('#(\\w+)', element):\n",
    "        count_hashtag[hashtags] += 1\n",
    "\n",
    "top10_hashtag = dict(sorted(count_hashtag.items(), key=itemgetter(1), reverse=True)[:10])\n",
    "top10_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLM': 9535,\n",
       " 'Resist': 6036,\n",
       " 'BlackLivesMatter': 4681,\n",
       " 'resist': 3797,\n",
       " 'FBR': 3239,\n",
       " 'TheResistance': 2995,\n",
       " 'blacklivesmatter': 2645,\n",
       " '1': 2627,\n",
       " 'Resistance': 1919,\n",
       " 'RESIST': 1823}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cher hashtags\n",
    "\n",
    "count_hashtag = Counter()\n",
    "for element in emoji_cher:\n",
    "    for hashtags in re.findall('#(\\w+)', element):\n",
    "        count_hashtag[hashtags] += 1\n",
    "\n",
    "top10_hashtag = dict(sorted(count_hashtag.items(), key=itemgetter(1), reverse=True)[:10])\n",
    "top10_hashtag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love: 6\n",
      "hang: 3\n",
      "girl: 3\n",
      "kills: 2\n",
      "time: 2\n"
     ]
    }
   ],
   "source": [
    "# robyn title\n",
    "\n",
    "robyn_title=lyrics_df.loc[lyrics_df['artist']=='robyn','lyrics']\n",
    "robyn_title=list(robyn_title)\n",
    "\n",
    "\n",
    "song_title=[]\n",
    "counter=0\n",
    "for i in robyn_title:\n",
    "    title=i.split('\\n')[0]\n",
    "    song_title.append(title)\n",
    "    counter+=1\n",
    "    \n",
    "\n",
    "song_title=str(song_title)\n",
    "t1=re.sub( ' \"  ','' ,song_title)\n",
    "t2=re_punctuations(t1)\n",
    "t3=t2.lower()\n",
    "t4=t3.split()\n",
    "t5= [word for word in t4 if not word in stopwords.words()]\n",
    "\n",
    "\n",
    "# top 5 words\n",
    "frequency={}\n",
    "for item in t5:\n",
    "   # checking the element in dictionary\n",
    "   if item in frequency:\n",
    "      # incrementing the counr\n",
    "      frequency[item] += 1\n",
    "   else:\n",
    "      # initializing the count\n",
    "      frequency[item] = 1\n",
    "    \n",
    "\n",
    "for key, value in sorted(frequency.items(), key=lambda kv: kv[1], reverse=True)[:5]:\n",
    "   print(\"%s: %s\" % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love: 38\n",
      "song: 11\n",
      "believe: 6\n",
      "time: 6\n",
      "heart: 6\n"
     ]
    }
   ],
   "source": [
    "# cher title\n",
    "\n",
    "cher_title=lyrics_df.loc[lyrics_df['artist']=='cher','lyrics']\n",
    "cher_title=list(cher_title)\n",
    "\n",
    "\n",
    "song_title=[]\n",
    "counter=0\n",
    "for i in cher_title:\n",
    "    title=i.split('\\n')[0]\n",
    "    song_title.append(title)\n",
    "    counter+=1\n",
    "    \n",
    "\n",
    "song_title=str(song_title)\n",
    "t1=re.sub( ' \"  ','' ,song_title)\n",
    "t2=re_punctuations(t1)\n",
    "t3=t2.lower()\n",
    "t4=t3.split()\n",
    "t5= [word for word in t4 if not word in stopwords.words()]\n",
    "\n",
    "\n",
    "# top 5 words\n",
    "frequency={}\n",
    "for item in t5:\n",
    "   # checking the element in dictionary\n",
    "   if item in frequency:\n",
    "      # incrementing the counr\n",
    "      frequency[item] += 1\n",
    "   else:\n",
    "      # initializing the count\n",
    "      frequency[item] = 1\n",
    "    \n",
    "\n",
    "for key, value in sorted(frequency.items(), key=lambda kv: kv[1], reverse=True)[:5]:\n",
    "     print(\"%s: %s\" % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Artist 1    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Artist 2    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A:  \\s+ will match one or more whitespace characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>includemeout</td>\n",
       "      <td>\"Include Me Out\"\\n\\n\\n\\nIt is really very simp...</td>\n",
       "      <td>[include, really, simple, single, pulse, repea...</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>electric</td>\n",
       "      <td>\"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...</td>\n",
       "      <td>[electric, electric, electric, natural, high, ...</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robyn</td>\n",
       "      <td>beach2k20</td>\n",
       "      <td>\"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...</td>\n",
       "      <td>[beach, 2k20, wanna, go, gonna, get, ok, call,...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn</td>\n",
       "      <td>lovekills</td>\n",
       "      <td>\"Love Kills\"\\n\\n\\n\\nIf you're looking for love...</td>\n",
       "      <td>[love, kills, youre, looking, love, get, heart...</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robyn</td>\n",
       "      <td>timemachine</td>\n",
       "      <td>\"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan...</td>\n",
       "      <td>[time, machine, hey, cant, believe, fit, threw...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist          song                                             lyrics  \\\n",
       "0  robyn  includemeout  \"Include Me Out\"\\n\\n\\n\\nIt is really very simp...   \n",
       "1  robyn      electric  \"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...   \n",
       "2  robyn     beach2k20  \"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...   \n",
       "3  robyn     lovekills  \"Love Kills\"\\n\\n\\n\\nIf you're looking for love...   \n",
       "4  robyn   timemachine  \"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan...   \n",
       "\n",
       "                                          clean_text  length  \n",
       "0  [include, really, simple, single, pulse, repea...     234  \n",
       "1  [electric, electric, electric, natural, high, ...     153  \n",
       "2  [beach, 2k20, wanna, go, gonna, get, ok, call,...     174  \n",
       "3  [love, kills, youre, looking, love, get, heart...     246  \n",
       "4  [time, machine, hey, cant, believe, fit, threw...     129  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lyric length comparison chart \n",
    "\n",
    "lyrics_df['length'] = lyrics_df['clean_text'].map(len)\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "cher     AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "robyn    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lyrics_df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "https://stackoverflow.com/questions/58485274/function-to-count-hashtags \n",
    "\n",
    "\n",
    "https://able.bio/rhett/sort-a-python-dictionary-by-value-or-key--84te6gv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
